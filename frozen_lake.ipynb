{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from time import sleep\n",
    "from ipythonblocks import BlockGrid\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 256)  # Input is the (x, y) position of the agent\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 4)  # Output is the Q-value for each action (up, down, left, right)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "# Agent class for DQN\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.model = DQN()\n",
    "        self.target_model = DQN()  # Target network for stable training\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.update_target_every = 10\n",
    "        self.steps = 0\n",
    "\n",
    "        self.name = \"Deep Q Agent\"\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return self.act(state)\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.choice([0, 1, 2, 3])  # Random action (explore)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()  # Choose action with highest Q-value (exploit)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            target = self.model(state)\n",
    "            with torch.no_grad():\n",
    "                target_next = self.target_model(next_state)\n",
    "\n",
    "            target[0][action] = reward + (self.gamma * torch.max(target_next)) * (1 - done)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss_fn(self.model(state), target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLMapperAgent():\n",
    "    def __init__(self, name:str = \"Mapped agent\"):\n",
    "        self.name = name\n",
    "\n",
    "    def get_action(self, state):\n",
    "        direction = {\"up\": 0, \"down\": 1, \"left\": 2, \"right\": 3}\n",
    "        choice = \"\"\n",
    "        match  tuple(state):\n",
    "            case (0, 0):\n",
    "                choice = \"left\"\n",
    "            case (1, 0):\n",
    "                choice = \"up\"\n",
    "            case (2, 0):\n",
    "                choice = \"up\"\n",
    "            case (3, 0):\n",
    "                choice = \"up\"\n",
    "            case (0, 1):\n",
    "                choice = \"left\"\n",
    "            case (2, 1):\n",
    "                choice = \"left\"\n",
    "            case (0, 2):\n",
    "                choice = \"up\"\n",
    "            case (1, 2):\n",
    "                choice = \"down\"\n",
    "            case (2, 2):\n",
    "                choice = \"left\"\n",
    "            case (1, 3):\n",
    "                choice = \"right\"\n",
    "            case (2, 3):\n",
    "                choice = \"down\"\n",
    "            case _:\n",
    "                choice = \"up\"\n",
    "        return direction[choice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLMoveRankAgent():\n",
    "    def __init__(self, model, name:str = \"Move ranking agent\"):\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        \n",
    "    def get_positions_from_choice(self, state, decision):\n",
    "        positions = []\n",
    "\n",
    "        new_position = state + self.model.decision_dict[decision]\n",
    "        if not ((new_position == (-1, -1)).any() or (new_position == (4, 4)).any()):\n",
    "            positions.append(new_position)\n",
    "        \n",
    "        current_pos = tuple(state)\n",
    "        # Can only slip perpendicular to direction move is attempted\n",
    "        if (decision == \"up\" or decision == \"down\"):\n",
    "            positions.append((min(current_pos[0] + 1, 3), current_pos[1]))\n",
    "            positions.append((max(current_pos[0] - 1, 0), current_pos[1]))\n",
    "        if (decision == \"left\" or decision == \"right\"):\n",
    "            positions.append((current_pos[0], min(current_pos[1] + 1, 3)))\n",
    "            positions.append((current_pos[0], max(current_pos[1] - 1, 0)))\n",
    "\n",
    "        return positions\n",
    "\n",
    "    def rate_position(self, position):\n",
    "        # Larger positions are closer to goal\n",
    "        score = position[0] + position[1]\n",
    "\n",
    "        if self.model.in_pit(np.array(position)):\n",
    "            score -= 150\n",
    "        if self.model.is_complete(np.array(position)):\n",
    "            score += 5000  # Goal reached\n",
    "\n",
    "        return score\n",
    "\n",
    "    def get_action(self, state):\n",
    "        direction_to_num = {\"up\": 0, \"down\": 1, \"left\": 2, \"right\": 3}\n",
    "\n",
    "        pos_next_positions = {\"up\": [], \"down\": [], \"left\": [], \"right\": []}\n",
    "        choice_scores = {\"up\": 0, \"down\": 0, \"left\": 0, \"right\": 0}\n",
    "        \n",
    "        for key in pos_next_positions.keys():\n",
    "            pos_next_positions[key] = self.get_positions_from_choice(state, key)\n",
    "            for pos in pos_next_positions[key]:\n",
    "                choice_scores[key] += self.rate_position(pos)\n",
    "\n",
    "        best = max(choice_scores, key=choice_scores.get)\n",
    "\n",
    "        choice_scores = dict(sorted(choice_scores.items(), key=lambda item: -item[1]))\n",
    "        \n",
    "        return direction_to_num[best]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLRecursiveRank(FLMoveRankAgent):\n",
    "    def __init__(self, model, name:str = \"Move ranking recursive agent\"):\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "\n",
    "    def rate_position(self, position, evaluated:dict = {}, alpha:float = 0.1, depth:int = 0, max_depth:int = 500):\n",
    "        if depth > max_depth:\n",
    "            return 0\n",
    "        else:\n",
    "            depth += 1\n",
    "\n",
    "        # Larger positions are closer to goal\n",
    "        score = position[0] + position[1]\n",
    "\n",
    "        if self.model.in_pit(np.array(position)):\n",
    "            score -= 150\n",
    "            return score\n",
    "        if self.model.is_complete(np.array(position)):\n",
    "            score += 5000  # Goal reached\n",
    "            return score\n",
    "        \n",
    "        if tuple(position) in evaluated.keys():\n",
    "            return evaluated[tuple(position)]\n",
    "\n",
    "        # Add fractional score of all subsequently reachable positions\n",
    "        for dir in [\"up\", \"down\", \"left\", \"right\"]:\n",
    "            for next_pos in self.get_positions_from_choice(position, dir):\n",
    "                score += self.rate_position(next_pos, depth=depth) * alpha\n",
    "        \n",
    "        # Maintain list of position - score pairs to speed up search\n",
    "        evaluated[tuple(position)] = score\n",
    "        return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLRandomAgent():\n",
    "    def __init__(self, name:str = \"Random agent\"):\n",
    "        self.name = name\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        return random.choice([0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLModel:\n",
    "    def __init__(self):\n",
    "        self.decision_dict = {\"up\": np.array((0, -1)), \"down\": np.array((0, 1)), \"left\": np.array((-1, 0)), \"right\": np.array((1, 0))}\n",
    "\n",
    "        self.goal_position = (3, 3)\n",
    "        self.pit_positions = ((1,1), (3,1), (3, 2), (0, 3))\n",
    "\n",
    "    def exog_info(self, state, decision):\n",
    "        if random.random() < 0.667:\n",
    "            return \"slip\"\n",
    "        else:\n",
    "            return \"move\"\n",
    "\n",
    "    def get_reward(self, state):\n",
    "        if self.in_pit(state):\n",
    "            return -15\n",
    "        if self.is_complete(state):\n",
    "            return 50  # Goal reached\n",
    "        \n",
    "        return -0.1  # Small penalty for each move\n",
    "    \n",
    "    def in_pit(self, state):\n",
    "        return tuple(state) in self.pit_positions\n",
    "\n",
    "    def transition_function(self, state, decision, exog_info):\n",
    "        if exog_info != \"slip\":\n",
    "            new_position = state + self.decision_dict[decision]\n",
    "            if not ((new_position == (-1, -1)).any() or (new_position == (4, 4)).any()):\n",
    "                return new_position\n",
    "        \n",
    "        current_pos = tuple(state)\n",
    "        # Can only slip perpendicular to direction move is attempted\n",
    "        possible_positions = []\n",
    "        if (decision == \"up\" or decision == \"down\"):\n",
    "            possible_positions.append((min(current_pos[0] + 1, 3), current_pos[1]))\n",
    "            possible_positions.append((max(current_pos[0] - 1, 0), current_pos[1]))\n",
    "        if (decision == \"left\" or decision == \"right\"):\n",
    "            possible_positions.append((current_pos[0], min(current_pos[1] + 1, 3)))\n",
    "            possible_positions.append((current_pos[0], max(current_pos[1] - 1, 0)))\n",
    "\n",
    "        slip_position = random.choice(possible_positions)\n",
    "        return np.array(slip_position)\n",
    "\n",
    "    def is_complete(self, state):\n",
    "        return (state == (3, 3)).all()\n",
    "\n",
    "    def draw_state(self, state):\n",
    "        agent_colour = (0, 0, 255)\n",
    "        goal_colour = (210, 30, 30)\n",
    "        pit_colour = (30, 30, 30)\n",
    "        clean_colour = (255, 255, 255)\n",
    "        \n",
    "        # Create the grid with the given width and height\n",
    "        grid = BlockGrid(4, 4, fill=clean_colour)\n",
    "        \n",
    "        agent_position = state\n",
    "        \n",
    "        # Set the agent's position with its color\n",
    "        grid[self.goal_position[1], self.goal_position[0]] = goal_colour\n",
    "        grid[int(agent_position[1]), int(agent_position[0])] = agent_colour\n",
    "\n",
    "        for pit in self.pit_positions:\n",
    "            grid[pit[1], pit[0]] = pit_colour\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Show the grid\n",
    "        grid.show()\n",
    "        \n",
    "        # Add a small delay for visualization purposes\n",
    "        sleep(0.5)\n",
    "\n",
    "    \n",
    "    # Method to run the trained agent in the environment and visualize\n",
    "    def view_trained_agent(self, S0, agent, n_iterations: int = 200):\n",
    "        current_state = S0\n",
    "        for n in range(n_iterations):\n",
    "            self.draw_state(current_state)  # Visualize the state\n",
    "            action = agent.get_action(current_state)  # Let the trained agent decide an action\n",
    "            action_name = [\"up\", \"down\", \"left\", \"right\"][action]  # Convert action index to action name\n",
    "            exog = self.exog_info(current_state, action_name)  # Get external environment info\n",
    "            current_state = self.transition_function(current_state, action_name, exog)  # Transition to next state\n",
    "            \n",
    "            if self.in_pit(current_state):  # Check if pit is hit\n",
    "                self.draw_state(current_state)  # Final state visualization\n",
    "                print(f\"Fell in pit on step {n+1}!\")\n",
    "                break\n",
    "\n",
    "            if self.is_complete(current_state):  # Check if goal is reached\n",
    "                self.draw_state(current_state)  # Final state visualization\n",
    "                print(f\"Goal reached in {n+1} steps!\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Agent did not reach the goal.\")\n",
    "\n",
    "    \n",
    "        # Method to run the trained agent in the environment and visualize\n",
    "    def test_agent(self, S0, agent, view: bool = True, n_iterations: int = 200):\n",
    "        total_completions = 0\n",
    "        for n in range(n_iterations):\n",
    "            current_state = S0\n",
    "            for i in range(200): # Allow up to 200 steps to reach goal\n",
    "                action = agent.get_action(current_state)  # Let the trained agent decide an action\n",
    "                action_name = [\"up\", \"down\", \"left\", \"right\"][action]  # Convert action index to action name\n",
    "                exog = self.exog_info(current_state, action_name)  # Get external environment info\n",
    "                current_state = self.transition_function(current_state, action_name, exog)  # Transition to next state\n",
    "                \n",
    "                if self.in_pit(current_state):  # Check if pit is hit\n",
    "                    break\n",
    "\n",
    "                if self.is_complete(current_state):  # Check if goal is reached\n",
    "                    total_completions += 1\n",
    "                    break\n",
    "        return total_completions/n_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_agent(model):\n",
    "#     # Training loop\n",
    "#     env = model\n",
    "#     agent = DQNAgent()\n",
    "#     start_state = np.array((0, 0))\n",
    "\n",
    "#     episode_rewards = []\n",
    "\n",
    "#     n_episodes = 1000\n",
    "#     for episode in range(n_episodes):\n",
    "#         state = start_state\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "\n",
    "#         while not done:\n",
    "#             action = agent.act(state)\n",
    "#             action_name = [\"up\", \"down\", \"left\", \"right\"][action]\n",
    "#             exog = env.exog_info(state, action_name)\n",
    "#             next_state = env.transition_function(state, action_name, exog)\n",
    "#             reward = env.get_reward(next_state)\n",
    "#             done = env.is_complete(next_state) or env.in_pit(next_state)\n",
    "            \n",
    "#             agent.remember(state, action, reward, next_state, done)\n",
    "#             state = next_state\n",
    "#             total_reward += reward\n",
    "\n",
    "#             if done:\n",
    "#                 print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "#                 episode_rewards.append(total_reward)\n",
    "#                 break\n",
    "\n",
    "#         agent.replay()\n",
    "#         if episode % agent.update_target_every == 0:\n",
    "#             agent.update_target_network()\n",
    "\n",
    "#     import matplotlib\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     # set up matplotlib\n",
    "#     is_ipython = 'inline' in matplotlib.get_backend()\n",
    "#     if is_ipython:\n",
    "#         from IPython import display\n",
    "\n",
    "#     plt.ion()\n",
    "\n",
    "#     # Plot the results\n",
    "\n",
    "#     # Function to calculate the running average\n",
    "#     def running_average(data, window_size=10):\n",
    "#         return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "#     # Compute running average of the last 10 rewards\n",
    "#     running_avg_rewards = running_average(episode_rewards, window_size=10)\n",
    "#     plt.plot(running_avg_rewards)\n",
    "#     plt.title('Running Average of Rewards (Window Size = 10)')\n",
    "#     plt.xlabel('Episodes')\n",
    "#     plt.ylabel('Average Reward (Last 10)')\n",
    "#     plt.ioff()\n",
    "#     plt.show()\n",
    "\n",
    "#     return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import torch\n",
    "\n",
    "# Function to visualize the current policy of the agent\n",
    "def visualize_policy(agent):\n",
    "    grid_size = 4\n",
    "    actions = ['↑', '↓', '←', '→']  # Symbols for up, down, left, right\n",
    "\n",
    "    # Initialize a grid to store the best action at each (x, y) position\n",
    "    policy_grid = np.empty((grid_size, grid_size), dtype=str)\n",
    "\n",
    "    # Loop over each state in the 4x4 grid\n",
    "    for x in range(grid_size):\n",
    "        for y in range(grid_size):\n",
    "            state = np.array([x, y])  # The state is the (x, y) position\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convert to tensor\n",
    "            q_values = agent.model(state_tensor)  # Get Q-values for this state from the model\n",
    "            best_action = torch.argmax(q_values).item()  # Get the action with the highest Q-value\n",
    "            policy_grid[y, x] = actions[best_action]  # Store the best action using arrows\n",
    "\n",
    "    # Create the plot to visualize the policy\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    for x in range(grid_size):\n",
    "        for y in range(grid_size):\n",
    "            plt.text(x, y, policy_grid[y, x], fontsize=20, ha='center', va='center')\n",
    "\n",
    "    # Set grid and labels\n",
    "    plt.xlim(-0.5, grid_size - 0.5)\n",
    "    plt.ylim(-0.5, grid_size - 0.5)\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to match usual grid layout\n",
    "    plt.grid(True)\n",
    "    plt.xticks(np.arange(grid_size))\n",
    "    plt.yticks(np.arange(grid_size))\n",
    "    plt.title(\"Best Action per State (x, y)\")\n",
    "    plt.show()\n",
    "\n",
    "# Main training function with policy visualization\n",
    "def train_agent(model):\n",
    "    # Training loop\n",
    "    env = model\n",
    "    agent = DQNAgent()\n",
    "    start_state = np.array((0, 0))\n",
    "\n",
    "    episode_rewards = []\n",
    "    n_episodes = 5000\n",
    "\n",
    "    # set up matplotlib for interactive plotting\n",
    "    plt.ion()\n",
    "    is_ipython = 'inline' in plt.get_backend()\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state = start_state\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            action_name = [\"up\", \"down\", \"left\", \"right\"][action]\n",
    "            exog = env.exog_info(state, action_name)\n",
    "            next_state = env.transition_function(state, action_name, exog)\n",
    "            reward = env.get_reward(next_state)\n",
    "            done = env.is_complete(next_state) or env.in_pit(next_state)\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                episode_rewards.append(total_reward)\n",
    "                break\n",
    "\n",
    "        agent.replay()\n",
    "        if episode % agent.update_target_every == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        # Plot only every 5 episodes\n",
    "        if (episode + 1) % 5 == 0:\n",
    "            plt.figure(1)\n",
    "            plt.clf()  # Clear the plot for fresh plotting\n",
    "            plt.title('Training...')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Total Reward')\n",
    "\n",
    "            # Convert episode_rewards to a tensor for plotting\n",
    "            rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "\n",
    "            # Plot episode rewards\n",
    "            plt.plot(rewards_t.numpy())\n",
    "\n",
    "            # Plot the running average of rewards\n",
    "            ave_over_last = 200\n",
    "            if len(rewards_t) >= ave_over_last:\n",
    "                running_avg_rewards = np.convolve(episode_rewards, np.ones(ave_over_last) / ave_over_last, mode='valid')\n",
    "                plt.plot(running_avg_rewards, label=f\"Running Avg (Last {ave_over_last})\", color='orange')\n",
    "\n",
    "            plt.pause(0.001)  # Pause to update the plot\n",
    "            if (episode + 1) % 50 == 0:\n",
    "                visualize_policy(agent)  # Visualize the policy\n",
    "\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "    # After training is done, turn off interactive mode and display the final result\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize the final policy\n",
    "    visualize_policy(agent)\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_lake_model = FLModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[508], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m         trained_agent \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     trained_agent \u001b[38;5;241m=\u001b[39m train_agent(frozen_lake_model)\n",
      "Cell \u001b[0;32mIn[506], line 74\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     71\u001b[0m         episode_rewards\u001b[38;5;241m.\u001b[39mappend(total_reward)\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m agent\u001b[38;5;241m.\u001b[39mreplay()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m agent\u001b[38;5;241m.\u001b[39mupdate_target_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     76\u001b[0m     agent\u001b[38;5;241m.\u001b[39mupdate_target_network()\n",
      "Cell \u001b[0;32mIn[499], line 64\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     63\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state), target)\n\u001b[0;32m---> 64\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_min:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:465\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_tensor_str\u001b[38;5;241m.\u001b[39m_str(\u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents)\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    467\u001b[0m ):\n\u001b[1;32m    468\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m            used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(agent):\n",
    "    with open(\"trained_agent.pkl\", \"ab\") as file:\n",
    "        pickle.dump(agent, file)\n",
    "\n",
    "load = False\n",
    "\n",
    "if load:\n",
    "    with open(\"trained_agent.pkl\", \"rb\") as file:\n",
    "        trained_agent = pickle.load(file)\n",
    "else:\n",
    "    trained_agent = train_agent(frozen_lake_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Q Agent completed 58.65% of attempts.\n",
      "Move ranking recursive agent completed 81.45% of attempts.\n",
      "Move ranking agent completed 38.45% of attempts.\n",
      "Random agent completed 1.95% of attempts.\n",
      "Mapped agent completed 82.6% of attempts.\n"
     ]
    }
   ],
   "source": [
    "random_agent = FLRandomAgent()\n",
    "mapped_agent = FLMapperAgent()\n",
    "rank_agent = FLMoveRankAgent(frozen_lake_model)\n",
    "rank_agent_recursive = FLRecursiveRank(frozen_lake_model)\n",
    "\n",
    "agents = [trained_agent, rank_agent_recursive, rank_agent, random_agent, mapped_agent]\n",
    "\n",
    "start_state = np.array((0, 0))\n",
    "for agent in agents:\n",
    "    print(f\"{agent.name} completed {round(frozen_lake_model.test_agent(start_state, agent, n_iterations=2000) * 100, 2)}% of attempts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**20000 attempts**\n",
    "\n",
    "Deep Q Agent completed 77.05% of attempts.\n",
    "\n",
    "Move ranking recursive agent completed 82.13% of attempts.\n",
    "\n",
    "Move ranking agent completed 39.45% of attempts.\n",
    "\n",
    "Random agent completed 1.45% of attempts.\n",
    "\n",
    "Mapped agent completed 81.79% of attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">table.blockgrid {border: none;} .blockgrid tr {border: none;} .blockgrid td {padding: 0px;} #blocks3b72dd41-5d9c-4b36-bf7c-8eaa627c63df td {border: 1px solid white;}</style><table id=\"blocks3b72dd41-5d9c-4b36-bf7c-8eaa627c63df\" class=\"blockgrid\"><tbody><tr><td title=\"Index: [0, 0]&#10;Color: (255, 255, 255)\" style=\"width: 20px; height: 20px;background-color: rgb(255, 255, 255);\"></td><td title=\"Index: [0, 1]&#10;Color: (255, 255, 255)\" style=\"width: 20px; height: 20px;background-color: rgb(255, 255, 255);\"></td><td title=\"Index: [0, 2]&#10;Color: (255, 255, 255)\" style=\"width: 20px; height: 20px;background-color: rgb(255, 255, 255);\"></td><td title=\"Index: [0, 3]&#10;Color: (255, 255, 255)\" style=\"width: 20px; height: 20px;background-color: rgb(255, 255, 255);\"></td></tr><tr><td title=\"Index: [1, 0]&#10;Color: (255, 255, 255)\" style=\"width: 20px; height: 20px;background-color: rgb(255, 255, 255);\"></td><td title=\"Index: [1, 1]&#10;Color: (30, 30, 30)\" style=\"width: 20px; height: 20px;background-color: rgb(30, 30, 30);\"></td><td title=\"Index: [1, 2]&#10;Color: (255, 255, 255)\" style=\"width: 20px; height: 20px;background-color: rgb(255, 255, 255);\"></td><td title=\"Index: [1, 3]&#10;Color: (30, 30, 30)\" style=\"width: 20px; height: 20px;background-color: rgb(30, 30, 30);\"></td></tr><tr><td title=\"Index: [2, 0]&#10;Color: (255, 255, 255)\" style=\"width: 20px; height: 20px;background-color: rgb(255, 255, 255);\"></td><td title=\"Index: [2, 1]&#10;Color: (255, 255, 255)\" style=\"width: 20px; height: 20px;background-color: rgb(255, 255, 255);\"></td><td title=\"Index: [2, 2]&#10;Color: (255, 255, 255)\" style=\"width: 20px; height: 20px;background-color: rgb(255, 255, 255);\"></td><td title=\"Index: [2, 3]&#10;Color: (30, 30, 30)\" style=\"width: 20px; height: 20px;background-color: rgb(30, 30, 30);\"></td></tr><tr><td title=\"Index: [3, 0]&#10;Color: (30, 30, 30)\" style=\"width: 20px; height: 20px;background-color: rgb(30, 30, 30);\"></td><td title=\"Index: [3, 1]&#10;Color: (255, 255, 255)\" style=\"width: 20px; height: 20px;background-color: rgb(255, 255, 255);\"></td><td title=\"Index: [3, 2]&#10;Color: (255, 255, 255)\" style=\"width: 20px; height: 20px;background-color: rgb(255, 255, 255);\"></td><td title=\"Index: [3, 3]&#10;Color: (210, 30, 30)\" style=\"width: 20px; height: 20px;background-color: rgb(210, 30, 30);\"></td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fell in pit on step 70!\n"
     ]
    }
   ],
   "source": [
    "frozen_lake_model.view_trained_agent(start_state, trained_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
